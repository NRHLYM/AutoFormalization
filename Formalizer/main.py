"""
Formalizer/main.py

é¡¹ç›®çš„ä¸»å…¥å£ (æ‰¹é‡å¤„ç†ç‰ˆ + è¿›åº¦ç»Ÿè®¡ + åŒè½¨æ—¥å¿—)ã€‚
- ç»ˆç«¯ (Console): åªæ˜¾ç¤ºè¿›åº¦å’Œç®€è¦ç»“æœ (INFO çº§åˆ«)ã€‚
- æ—¥å¿— (File): ä¿å­˜æ‰€æœ‰è¯¦ç»†çš„ Promptã€ä»£ç å’Œè°ƒè¯•ä¿¡æ¯ (DEBUG çº§åˆ«)ã€‚
"""

import sys
import os
import json
import argparse
import traceback
import logging
from datetime import datetime

# ç¡®ä¿ 'modules' å¯ä»¥è¢«å¯¼å…¥
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from stage1_planner import GoTPlanner
    from stage2_synthesizer import GoTSynthesizer
    from stage3_alignment import SemanticAlignmentModule
    from modules.logger_setup import setup_logging # ç¡®ä¿ä½ åˆ›å»ºäº†è¿™ä¸ªæ–‡ä»¶
    import config
except ImportError as e:
    print(f"é”™è¯¯: æ— æ³•å¯¼å…¥å¿…è¦çš„æ¨¡å—ã€‚{e}")
    print("è¯·æ£€æŸ¥æ˜¯å¦å·²åˆ›å»º 'modules/logger_setup.py' å¹¶ä¸”å…¶ä»–æ¨¡å—éƒ½åœ¨æ­£ç¡®ä½ç½®ã€‚")
    exit(1)

def save_individual_result(output_dir, index, code, status_report):
    """ä¿å­˜å•ä¸ªé—®é¢˜çš„ Lean ä»£ç å’Œå…ƒæ•°æ®"""
    # 1. ä¿å­˜ä»£ç 
    lean_filename = os.path.join(output_dir, f"problem_{index}.lean")
    try:
        with open(lean_filename, "w", encoding="utf-8") as f:
            f.write(code)
    except IOError: pass

    # 2. ä¿å­˜æŠ¥å‘Š
    meta_filename = os.path.join(output_dir, f"problem_{index}_report.json")
    try:
        with open(meta_filename, "w", encoding="utf-8") as f:
            json.dump(status_report, f, indent=2, ensure_ascii=False)
    except: pass


def process_single_problem(entry: dict, output_dir: str, image_root_dir: str = None) -> dict:
    """
    å¤„ç†å•ä¸ªé—®é¢˜ã€‚è¿”å›ç»“æœæ‘˜è¦ dictã€‚
    """
    idx = entry.get("index", "unknown")
    question = entry.get("question", "")
    category = entry.get("category", "Unknown")
    image_file = entry.get("image")

    real_image_path = None
    if image_file and image_root_dir:
        potential_path = os.path.join(image_root_dir, image_file)
        if os.path.exists(potential_path):
            real_image_path = potential_path
            logging.info(f" [Image] å‘ç°å…³è”å›¾ç‰‡: {real_image_path}")
        else:
            logging.warning(f" [Image] âš ï¸ å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°: {potential_path}")


    # Stage 1 (åˆ†è§£) & Stage 2 (åˆæˆ):
    gen_image_path = real_image_path if config.USE_MULTIMODAL else None

    # Stage 3 (è¯­ä¹‰æ£€æµ‹):
    check_image_path = real_image_path

    # [INFO] æ‰“å°ä»»åŠ¡æ¨¡å¼ä¿¡æ¯
    logging.info(f"\n{'=' * 60}")
    logging.info(f" [Task Start] Index: {idx} | Category: {category}")
    logging.info(f" Mode: {'Multimodal' if config.USE_MULTIMODAL else 'Text-Only'}")
    if real_image_path:
        logging.info(f" Logic: ç”Ÿæˆ{'çœ‹' if gen_image_path else 'ä¸çœ‹'}å›¾, æ£€æµ‹å¼ºåˆ¶çœ‹å›¾")
    logging.info(f" Question: {question[:80]}...")
    logging.info(f"{'=' * 60}")

    result_summary = {
        "index": idx,
        "question": question,
        "status": "failed",  # æœ€ç»ˆå¤§çŠ¶æ€
        "compilation_passed": False,  # ç¼–è¯‘æ˜¯å¦é€šè¿‡
        "semantic_passed": False,  # è¯­ä¹‰æ˜¯å¦é€šè¿‡
        "error": None,
        "consistency_level": "N/A",
        "generated_code": ""
    }

    try:
        # --- é˜¶æ®µä¸€ï¼šGoT åˆ†è§£ ---
        logging.info(f"\n--- [P{idx} é˜¶æ®µä¸€ï¼šåˆ†è§£] ---")
        planner = GoTPlanner()
        graph = planner.run(question, image_path=gen_image_path)

        # --- é˜¶æ®µäºŒï¼šGoT åˆæˆ ---
        logging.info(f"\n--- [P{idx} é˜¶æ®µäºŒï¼šåˆæˆ] ---")
        synthesizer = GoTSynthesizer()
        final_lean_code, synthesized_cache = synthesizer.run(graph, image_path=gen_image_path)
        result_summary["generated_code"] = final_lean_code

        # æ£€æŸ¥é˜¶æ®µäºŒæ˜¯å¦å‘ç”Ÿè‡´å‘½é”™è¯¯ (æˆªæ–­é€»è¾‘)
        if "-- FATAL:" in final_lean_code:
            logging.warning(f"!! [P{idx}] é˜¶æ®µäºŒåˆæˆå¤±è´¥ (ç¼–è¯‘æœªé€šè¿‡)ã€‚")
            result_summary["error"] = "Stage 2 Synthesis Failed"
            result_summary["compilation_passed"] = False
            save_individual_result(output_dir, idx, final_lean_code, result_summary)
            return result_summary

        result_summary["compilation_passed"] = True

        # --- é˜¶æ®µä¸‰ï¼šè¯­ä¹‰å¯¹é½ (ASCC) ---
        logging.info(f"\n--- [P{idx} é˜¶æ®µä¸‰ï¼šå¯¹é½] ---")
        aligner = SemanticAlignmentModule()

        is_consistent, report = aligner.run(
            question,
            synthesized_cache,
            graph,
            image_path=check_image_path
        )

        consistency_level = report.get("consistency_level", "level_3")
        result_summary["consistency_level"] = consistency_level
        result_summary["ascc_report"] = report

        if is_consistent:
            result_summary["status"] = "success"
            result_summary["semantic_passed"] = True
            logging.info(f"âœ… [P{idx}] å®Œç¾é€šè¿‡ï¼(Level: {consistency_level})")
        else:
            result_summary["status"] = "inconsistent"
            result_summary["semantic_passed"] = False
            logging.info(f"âš ï¸ [P{idx}] ç¼–è¯‘é€šè¿‡ä½†è¯­ä¹‰ä¸ä¸€è‡´ (Level: {consistency_level})")

        # ä¿å­˜æ­¤é¢˜çš„æ–‡ä»¶
        save_individual_result(output_dir, idx, final_lean_code, result_summary)

    except Exception as e:
        err_msg = traceback.format_exc()
        # [ERROR] ç®€ç•¥æŠ¥é”™è¿›ç»ˆç«¯ï¼Œè¯¦ç»†å †æ ˆè¿›æ—¥å¿—æ–‡ä»¶
        logging.error(f"!! [P{idx}] å¤„ç†å¼‚å¸¸: {e}")
        logging.debug(f"è¯¦ç»†å †æ ˆ:\n{err_msg}")

        result_summary["status"] = "error"
        result_summary["error"] = str(e)

    return result_summary

def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡è¿è¡Œå·¥å…·")
    parser.add_argument("--input", type=str, default="data.jsonl", help="è¾“å…¥æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default=None, help="æŒ‡å®šè¾“å‡ºç›®å½•")
    parser.add_argument("--limit", type=int, default=-1, help="ä»…è¿è¡Œå‰ N ä¸ªä»»åŠ¡")
    parser.add_argument("--multimodal", action="store_true", help="å¼€å¯å¤šæ¨¡æ€ (è¯»å– images/)")
    args = parser.parse_args()

    if args.multimodal:
        config.USE_MULTIMODAL = True
        logging.info("[Config] å¤šæ¨¡æ€å·²å¼€å¯ ğŸ–¼ï¸")

    # 1. è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        run_output_dir = args.output_dir
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        run_output_dir = os.path.join(config.BASE_DIR, "batch_results", timestamp)
    os.makedirs(run_output_dir, exist_ok=True)

    # 2. åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ (Log Setup)
    # è¯¦ç»†æ—¥å¿—ä¿å­˜åˆ° out.log
    log_file = os.path.join(run_output_dir, "out.log")
    setup_logging(log_file)

    logging.info(f"[BatchRunner] è¾“å‡ºç›®å½•: {run_output_dir}")
    logging.info(f"[BatchRunner] è¯¦ç»†æ—¥å¿—: {log_file}")

    # 3. è¯»å–æ•°æ®
    if not os.path.exists(args.input):
        logging.error(f"é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {args.input}")
        return

    tasks = []
    with open(args.input, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                tasks.append(json.loads(line))

    # å¤„ç† limit å‚æ•°
    if args.limit > 0:
        tasks = tasks[:args.limit]
        logging.info(f"[BatchRunner] å·²é™åˆ¶è¿è¡Œå‰ {args.limit} ä¸ªä»»åŠ¡")

    total_tasks = len(tasks)
    logging.info(f"[BatchRunner] ä»»åŠ¡æ€»æ•°: {total_tasks}")

    # 4. ç»Ÿè®¡å˜é‡
    compiled_count = 0
    semantic_count = 0

    summary_file = os.path.join(run_output_dir, "summary.jsonl")

    # 5. ä¸»å¾ªç¯
    with open(summary_file, "w", encoding="utf-8") as f_out:
        for i, entry in enumerate(tasks):
            current_idx = i + 1

            input_abs_path = os.path.abspath(args.input)
            input_dir = os.path.dirname(input_abs_path)

            # è¿™é‡Œå®šä¹‰å˜é‡åï¼Œæ¯”å¦‚å« image_search_path
            image_search_path = os.path.join(input_dir, "image")
            # æ‰§è¡Œå•ä¸ªä»»åŠ¡
            res = process_single_problem(entry, run_output_dir, image_root_dir=image_search_path)

            # æ›´æ–°ç»Ÿè®¡
            if res["compilation_passed"]:
                compiled_count += 1
            if res["semantic_passed"]:
                semantic_count += 1

            # å®æ—¶å†™å…¥ç»“æœæ‘˜è¦
            f_out.write(json.dumps(res, ensure_ascii=False) + "\n")
            f_out.flush()

            # --- å®æ—¶è¿›åº¦æ˜¾ç¤º ---
            comp_rate = (compiled_count / current_idx) * 100
            sem_rate = (semantic_count / current_idx) * 100

            logging.info("\n" + "-"*60)
            logging.info(f"ğŸ“Š [å®æ—¶ç»Ÿè®¡] è¿›åº¦: {current_idx}/{total_tasks}")
            logging.info(f"   ğŸ”¨ ç¼–è¯‘é€šè¿‡: {compiled_count}/{current_idx} ({comp_rate:.1f}%)")
            logging.info(f"   âœ… è¯­ä¹‰é€šè¿‡: {semantic_count}/{current_idx} ({sem_rate:.1f}%)")
            logging.info("-"*60 + "\n")

    # 6. æœ€ç»ˆæ€»ç»“
    logging.info(f"\n{'='*60}")
    logging.info(f" ğŸ‰ æ‰¹é‡è¿è¡Œç»“æŸ")
    logging.info(f" æ€»ä»»åŠ¡æ•°: {total_tasks}")
    logging.info(f" æœ€ç»ˆç¼–è¯‘æˆåŠŸç‡: {compiled_count}/{total_tasks} ({(compiled_count/total_tasks)*100:.1f}%)")
    logging.info(f" æœ€ç»ˆè¯­ä¹‰é€šè¿‡ç‡: {semantic_count}/{total_tasks} ({(semantic_count/total_tasks)*100:.1f}%)")
    logging.info(f" ç»“æœä¿å­˜åœ¨: {run_output_dir}")
    logging.info(f"{'='*60}")

if __name__ == "__main__":
    main()